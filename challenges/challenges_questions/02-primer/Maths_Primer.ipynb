{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2 & 3 Compatibility\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maths You Need to Know (and some you don't)\n",
    "\n",
    "You can use this as a reference more than anything..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "### Functions\n",
    "In mathematics, a **function** is simply a mapping from an input variable (or variables) to an output variable.  That is, it ***uniquely*** specifies a value for the output variable (the **range**) for all possible values of the input variables (the **domain**).\n",
    "\n",
    "##### Notation\n",
    "We will indicate a function, $f$, of one or more input variables $\\{x_1, x_2, \\; ... \\; , x_n\\}$ as:\n",
    "$$ \n",
    "\\bbox[aqua, 8px]{\n",
    "f(x_1, x_2, \\; ... \\;, x_n)\n",
    "}\n",
    "$$\n",
    "\n",
    "#### Visualizing Functions\n",
    "- Functions of 1 input variable can be plotted in a 2-D plane\n",
    "- Functions of 2 input variables can be plotted in a 3-D coordinate system\n",
    "- Anything more, and us poor humans are generally out of luck\n",
    "  - Most functions in Machine Learning deal in far more than 2 input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Functions\n",
    "\n",
    "##### Polynomials\n",
    "- Polynomials are any function of the form: $f(x) = \\sum\\limits_{i=0}^{n}a_ix^i$\n",
    "  - The $a_i$ are (possibly zero) **coefficients** for each term\n",
    "- The highest integer exponent on $x$ in $f(x)$ is the **degree** of the polynomial\n",
    "- Polynomials can extend to multivariable functions as well\n",
    "  - e.g.: $f(x_1, x_2) = x_1^2x_2 - 4x_2 + x_1^3$\n",
    "\n",
    "Let's try plotting some polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Several Polynomials in 1-D and 2-D\n",
    "x = np.linspace(0, 2, num=100)\n",
    "# Plot Quadratic Function\n",
    "f_quadratic = x**2\n",
    "# Plot Cubic Function\n",
    "f_cubic = (x-1)**3\n",
    "# Plot Quartic Function\n",
    "f_quartic1 = (x-1)**2*x*(x-2)\n",
    "# Plot Quartic Function\n",
    "f_quartic2 = (x**4)/3.5\n",
    "# Plot Quintic Function\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "plt.plot(x, f_quadratic, label=r'$x^2$')\n",
    "plt.plot(x, f_cubic, label=r'$(x-1)^3$')\n",
    "plt.plot(x, f_quartic1, label=r'$(x-1)^2x(x-2)$')\n",
    "plt.plot(x, f_quartic2, label=r'$x^4/3.5$')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The maximum number of **turning points** for a polynomial function is $degree-1$ (e.g. 3 for 4th degree, 2 for 3rd, etc).  A turning point is a local maximum or minimum, or equivalently where the function changes from increasing to decreasing or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nth Roots\n",
    "- These are functions s.t.: $f(x)^n = x$, so $f(x) = x^{1/n}$\n",
    "- These functions may not be defined over all values of $x$\n",
    "  - e.g.: $f(x) = \\sqrt{x} = x^{1/2}$ is not valid for $x < 0$ in the real number world\n",
    "\n",
    "Let's plot some nth roots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting nth roots\n",
    "x = np.linspace(0, 2, num=100)\n",
    "# Square root\n",
    "f1 = x**0.5\n",
    "# 4th root\n",
    "f2 = (x)**0.25\n",
    "# 10th root\n",
    "f3 = (x)**0.1\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "plt.plot(x, f1, label=r'$\\sqrt{x}$')\n",
    "plt.plot(x, f2, label=r'$x^{1/4}$')\n",
    "plt.plot(x, f3, label=r'$x^{1/10}$')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Power Functions\n",
    "- These are functions of the form $f(x) = a^x$ where $a$ is some constant\n",
    "- $a^0 = 1 \\; \\forall \\; a$\n",
    "\n",
    "Let's plot some power functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot power functions\n",
    "x = np.linspace(0, 2, num=100)\n",
    "f1 = 2**x\n",
    "f2 = 3**x\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "plt.plot(x, f1, label=r'$2^x$')\n",
    "plt.plot(x, f2, label=r'$3^x$')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logarithmic Function\n",
    "- These are functions of the form $f(x) = \\log_a(x)$, where $a$ is some constant\n",
    "\n",
    "**Properties of Logarithms**:  \n",
    "- A logarithm is defined s.t. it basically performs the inverse of $a^x$\n",
    "  - $\\log_a(a^x) = x$\n",
    "  - The constant $a$ is called the **base** of the logarithm\n",
    "- The most common integer bases are 2 (base 2 for computers), and 10 (not really, but we do use base 10)\n",
    "  - If you just see $\\log$ it's usually assumed to be one of those 2 bases\n",
    "- $\\log(c \\times d) = \\log(c) + \\log(d)$\n",
    "- $\\log(c / d) = \\log(c) - \\log(d)$\n",
    "- $\\log(c ^ d) = d\\log(c)$\n",
    "- $\\log(1) = 0$\n",
    "- $\\log(< 1) < 0$\n",
    "- $\\log_a(a) = 1$\n",
    "- $a^{log_ax} = x$\n",
    "\n",
    "Let's plot some logarithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Logs\n",
    "x = np.linspace(0, 10, num=100)\n",
    "# Base 10\n",
    "f10 = np.log10(x)\n",
    "# Base 2\n",
    "f2 = np.log2(x)\n",
    "# Natural Log\n",
    "fe = np.log(x)\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "plt.plot(x, f10, label=r'Base 10')\n",
    "plt.plot(x, f2, label=r'Base 2')\n",
    "plt.plot(x, fe, label=r'Natural Log')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Euler's Special Number $e$ and the Natural Log $ln$\n",
    "- A very special logarithm base is the number $e\\approx2.71828$\n",
    "- $e$ is a very special number that pops up everywhere\n",
    "- $e$ has several definitions:\n",
    "  - $e = \\sum\\limits_{i=0}{\\infty}\\frac{1}{n!}$\n",
    "  - $e = \\lim\\limits_{n \\to \\infty}(1 + \\frac{1}{n})^n$\n",
    "- The **natural log**, $ln$, is the **logarithm of base $e$**:\n",
    "  - $ln(e) = 1$\n",
    "  - $ln(e^x) = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trigonometric Functions\n",
    "- Trigonometric functions are a special class of **periodic functions**\n",
    "- **Periodic Functions**: repeat a certain pattern over and over\n",
    "- These functions have many practical applications in geometry, engineering, science, etc\n",
    "\n",
    "Here are 3 of the most common trigonometric functions:\n",
    "- **sine**: $\\sin(x)$\n",
    "- **cosine**: $\\cos(x)$\n",
    "- **tangent**: $\\tan(x)$\n",
    "\n",
    "There are 3 more, that are just their reciprocals:\n",
    "- **cosecant**: $\\csc(x) = \\frac{1}{\\sin(x)}$\n",
    "- **secant**: $\\sec(x) = \\frac{1}{\\cos(x)}$\n",
    "- **cotangent**: $\\cot(x) = \\frac{1}{\\tan(x)}$\n",
    "\n",
    "We could write down some infinite series to define them, but instead let's just take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Several Trig functions\n",
    "x = np.linspace(0, 2*np.pi, num=100)\n",
    "# Plot Sine Function\n",
    "fsin = np.sin(x)\n",
    "# Plot Cosine Function\n",
    "fcos = np.cos(x)\n",
    "# Plot Tangent Function\n",
    "#ftan = np.tan(x)\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "plt.plot(x, fsin, label=r'sine')\n",
    "plt.plot(x, fcos, label=r'cosine')\n",
    "#plt.plot(x, ftan, label=r'tangent')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperbolic Functions\n",
    "The trigonometric functions have some cousins, the hyperbolic functions, which are the following:\n",
    "- **Hyperbolic sine**: $\\sinh(x) = \\frac{e^x - e^{-x}}{2}$\n",
    "- **Hyperbolic cosine**: $\\cosh(x) = \\frac{e^x + e^{-x}}{2}$\n",
    "- **Hyperbolic tangent**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "And their reciprocals:\n",
    "- **Hyperbolic cosecant**: $csch(x) = \\frac{1}{\\sinh(x)}$\n",
    "- **Hyperbolic secant**: $sech(x) = \\frac{1}{\\cosh(x)}$\n",
    "- **Hyperbolic cotangent**: $\\coth(x) = \\frac{1}{\\tanh(x)}$\n",
    "\n",
    "Here's what these basically look like:\n",
    "<img src='img/hyperbolic.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Piecewise Functions\n",
    "- Piecewise functions can be defined by different relationships for different ranges of the input variables\n",
    "- e.g.:\n",
    "$$\n",
    "f(x) = \\begin{cases} x & x \\gt 3 \\\\ x^2 & x \\ge 3 \\end{cases}\n",
    "$$\n",
    "\n",
    "**Example: The Step Function**\n",
    "- The step function is simple, it is:\n",
    "$$\n",
    "f(x) = \\begin{cases} 0 & x \\lt 0 \\\\ 1 & x \\ge 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "Note that there is a **discontinuity** at $x=0$.  Nowhere have we said that functions need to be continuous!\n",
    "\n",
    "Here's what it looks like:\n",
    "<img src='img/step.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Function\n",
    "- A gaussian function is the familiar **normal** or **bell** curve and takes the form:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n",
    "$$\n",
    "- $\\mu$ is the **mean** of $x$ when dealing with a probability distributions\n",
    "  - It is also the center of the peak of the bell curve\n",
    "- $\\sigma$ is the **standard deviation** and controls the width of the curve\n",
    "\n",
    "Here's what it looks like:\n",
    "<img src='img/gaussian.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Function\n",
    "- Has the form:\n",
    "$$\n",
    "f(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "- This will be valuable next week ;)\n",
    "\n",
    "Here's what it looks like:\n",
    "<img src='img/sigmoid.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monotonicity and Monotonic Transformations\n",
    "- A function is **Monotonic** if:\n",
    "  - It's Monotonically Increasing: $c \\ge d \\rightarrow f(c) \\ge f(d) \\; \\forall \\; c, d$, OR\n",
    "  - It's Monotonically Decreasing: $c \\le d \\rightarrow f(c) \\le f(d) \\; \\forall \\; c, d$\n",
    "- **Monotonic Transformation**: A transformation of a monotonic function which maintains its monotonicity\n",
    "- **Logarithms** are a classic example of this, and this can be quite helpful!\n",
    "\n",
    "If the equations, don't help you, maybe a picture will:\n",
    "<img src='img/monotonic.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus\n",
    "- Calculus is the study of rates of change of variables with respect to other variables.\n",
    "  - e.g.: For a given change in $x$, what is the subsequent change in $f(x)$?\n",
    "- It's (probably) not overwhelmingly important that you're an expert for Data Science.\n",
    "- It **IS** important that you know a few of calculus's most important abilities, most notably:\n",
    "  - **Optimizing Functions**: finding the maximum(s) or minimum(s)\n",
    "  - Calculating **Average values** of a function\n",
    "  - For probability, normalizing to yield valid probabilities that obey the **Total Probability Theorem**\n",
    "\n",
    "### Differentiation\n",
    "\n",
    "#### Definition:\n",
    "- Differentiation means taking a **derivative**\n",
    "- A **derivative** of a function $f(x)$ **with respect to $x$** is the change in $f(x)$ caused by a change in $x$\n",
    "- Formally:\n",
    "$$\n",
    "\\frac{dy}{dx} = \\lim\\limits_{\\delta x \\to 0} \\frac{f(x+\\delta x) - f(x)}{\\delta x}\n",
    "$$\n",
    "\n",
    "#### Functions of a Single Variable:\n",
    "- For a single-variable function, this can be thought of as the **slope** of $f(x)$ at every point\n",
    "  - Recall the formula for **slope**: $\\Delta f / \\Delta x$\n",
    "  - As these changes in $x$ and subsequently $f$ get arbitrarily small--**infinitesimal**--we call them $\\delta f$ and $\\delta x$ and then $df$, $dx$ (\"**differentials**\") and arrive at the derivative\n",
    "- The derivative of $f(x)$ is a function itself\n",
    "  - Its value at any point $x=c$ represents the \"slope\" of $f(x)$ when $x=c$\n",
    "- Visually:\n",
    "<img src='img/derivative.png'/>\n",
    "\n",
    "##### Notation\n",
    "- You might see the derivative written as $f'(x)$, read \"f prime of x\"\n",
    "- Or more formally:\n",
    "$$\n",
    "\\bbox[aqua, 8px]{\n",
    "f'(x) = \\frac{dy}{dx}\n",
    "}\n",
    "$$\n",
    "- You can take a derivative of the derivative to get the second derivative: $f''(x)$, or:\n",
    "$$\n",
    "\\bbox[aqua, 8px]{\n",
    "f''(x) = \\frac{d^2y}{dx^2}\n",
    "}\n",
    "$$\n",
    "  \n",
    "#### Functions of Multiple Variables\n",
    "- For functions of multiple variables $f(x_1, x_2, \\; ... \\; , x_n)$, the derivatives are taken with respect to one variable at a time\n",
    "  - This is called a **partial derivative**\n",
    "  - Each partial derivative represents the rate of change of $f$ with respect to one variable, $x_i$, with the rest of the variables held constant\n",
    "  \n",
    "##### Notation\n",
    "- **Partial Derivatives** w.r.t. variable $x_i$, $x_j$\n",
    "  - **First Partial Derivative**: $f_{x_i} = \\frac{\\partial f}{\\partial x_i}$\n",
    "  - **Second Partial Derivative**: $f_{x_ix_i} = \\frac{\\partial^2 f}{\\partial x_i^2}$\n",
    "  - **Mixed Partial Derivatives**: $f_{x_ix_j} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$\n",
    "  \n",
    "#### Optimizing Functions - Finding the Maximum or Minimum Value\n",
    "The most handy use for calculus is probably finding the maximum or minimum value of a function and the value(s) of the input variable(s) that generate them.  Let's see how:\n",
    "\n",
    "##### Checking Understanding: Thought Experiment\n",
    "1. For a single-variable function $f(x)$, if we're at a point $x=c$ such that $f'(c) > 0$, how should we change the value of $x$ to raise the value of $f(x)$?\n",
    "2. To lower it?\n",
    "3. What if now at point $x=d$ we have $f'(d)=0$?  Will changing the value of $x$ raise or lower $f(x)$? (Hint: trick question)\n",
    "4. For a multi-variable function $f(x_1, x_2)$, same questions (for 3, assume **both** $\\frac{\\partial f}{\\partial x_1}=0$ and $\\frac{\\partial f}{\\partial x_2}=0$)\n",
    "\n",
    "##### Definitions:\n",
    "- **Local Optimum (Maximum or Minimum)**: The maximal/minimal value of $f(x)$ in a local region of $x$ space, or, a peak/valley in $f(x)$\n",
    "- **Global Optimum (Maximum or Minimum)**: The maximal/minimal value of $f(x)$ over all $x$\n",
    "- **Saddle Point**: A value of $x$ where the slope of $f(x)$ is 0 but it's neither a local maximum nor minimum\n",
    "- Visually:\n",
    "<img src='img/optimums.png'/img>\n",
    "\n",
    "##### Important Law\n",
    "- For a single-variable function $f(x)$ at a point $x=c$:\n",
    "  - If $f'(c)=0$, then $f(c)$ is one of the following:\n",
    "    - Local Maximum: if $f''(c)<0$\n",
    "    - Local Minimum: if $f''(c)>0$\n",
    "    - Saddle Point: if $f''(c)=0$\n",
    "\n",
    "**How does this help us?**  \n",
    "- To find local maxima we need $f'(c)=0$ **and** $f''(c) < 0$\n",
    "- To find local minima we need $f'(c)=0$ **and** $f''(c) > 0$\n",
    "- We simply find all those points, and take the largest/smallest value of $f$ to find the global maximum/minimum\n",
    "\n",
    "**Extending to Multivariable:**  \n",
    "- For a multivariable function $f(x_1, x_2)$ at a point $(c_1, c_2)$:\n",
    "  - If $\\frac{\\partial f}{\\partial x_i}(c_i) = 0 \\; \\forall \\; i$, then we have:\n",
    "    - $g(x_1, x_2) = f_{x_1x_1}f_{x_2x_2}-f_{x_1x_2}^2$\n",
    "    - Local Maximum: if $f_{x_1x_1} \\lt 0$ **AND** $g \\gt 0$\n",
    "    - Local Minimum: if $f_{x_1x_1} \\gt 0$ **AND** $g \\gt 0$\n",
    "    - Saddle Point: if $g \\lt 0$\n",
    "    - No clue: if $g = 0$\n",
    "    \n",
    "##### Checking Understanding\n",
    "This means we can (in theory) find the global minimum or maximum of any function that we can take an analytical derivative of--can you think of any place where this might come in handy from the last week?\n",
    "  \n",
    "#### Derivatives Rules and Common Functions\n",
    "- **Linearity**: $h(x) = af(x) + bg(x) \\rightarrow h'(x) = af'(x) + bg'(x)$, $a, b$ constants\n",
    "- **Power Rule**: $f(x) = x^r \\rightarrow f'(x) = rx^{r-1}$, $r$ a constant\n",
    "- **Power Functions**: $f(x) = a^x \\rightarrow f'(x) = \\ln(a)a^x$\n",
    "  - **Exponential Special Case**: $f(x) = e^x \\rightarrow f'(x) = e^x$\n",
    "- **Logarithmic Functions**: $f(x) = \\log_a(x) \\rightarrow f'(x) = \\frac{1}{x\\ln(a)}$\n",
    "  - **Natural Log Special Case**: $f(x) = \\ln(x) \\rightarrow f'(x) = \\frac{1}{x}$\n",
    "- **Trigonometric Functions**:\n",
    "  - $f(x) = \\sin(x) \\rightarrow f'(x) = \\cos(x)$\n",
    "  - $f(x) = \\cos(x) \\rightarrow f'(x) = -\\sin(x)$\n",
    "  - $f(x) = \\tan(x) \\rightarrow f'(x) = \\sec^2(x)$\n",
    "  - Good enough for now\n",
    "- **Sigmoid**: $f(x) = \\frac{1}{1 + e^{-x}} \\rightarrow f'(x) = f(1-f)$\n",
    "  - Derivative can be easily written in terms of original function, **useful!**\n",
    "- **Product Rule**: $h(x) = f(x)g(x) \\rightarrow h'(x) = f'(x) g(x) + g'(x)f(x)$\n",
    "- **Quotient Rule**: $h(x) = \\frac{f(x)}{g(x)} \\rightarrow h'(x) = \\frac{f'g - g'f}{g^2}$\n",
    "- **Chain Rule**: $h(x) = f(g(x)) \\rightarrow h'(x) = f'(g(x))g'(x)$\n",
    "  - e.g.: $h(x) = \\sin(2x^2+4x+3) \\rightarrow h'(x) = (4x + 4)\\cos(2x^2+4x+3)$\n",
    "  - $g(x) = 2x^2+4x+3$ and $f(x) = \\sin(x)$\n",
    "  \n",
    "\n",
    "### Integration\n",
    "\n",
    "#### Definition and Interpretation\n",
    "Integration is essentially the opposite of differentiation.  If differentiating a function $f(x)$ yields $f'(x)$, then integrating $f'(x)$ yields $f(x)$ (up to some additive constant).\n",
    "\n",
    "This is the celebrated **Fundamental Theorem of Calculus**:\n",
    "- If we have functions $F(x)$ and $f(x)$ s.t. $f(x) = F'(x)$, then:\n",
    "$$\n",
    "\\bbox[aqua, 8px]{\n",
    "\\int\\limits_{a}^{b}f(x)dx = F(b) - F(a)\n",
    "}\n",
    "$$\n",
    "\n",
    "##### Notation\n",
    "- The symbol $\\int$ represents taking an integral, or basically antiderivative\n",
    "\n",
    "##### Integrals as Area Under the Curve\n",
    "- We saw that derivatives can be roughly interpreted as the \"slope\" at a given point\n",
    "- **Integrals over a range compute the area under a curve (in 1-D)**\n",
    "- You can imagine taking smaller and smaller boxes of width $\\delta x$ and height $f(x)$ and summing their areas ($f(x)\\delta x$) over all $x$\n",
    "  - Area $\\approx \\sum\\limits_{i}f(x)\\delta x$\n",
    "  - As $\\delta x$ gets infinitesimally small, the summation yields the exact area and becomes an **integral**\n",
    "  - Visually:\n",
    "  <img src='img/integral.png'/>\n",
    "\n",
    "#### Common Integration Rules\n",
    "- **Linearity**: $\\int(af(x) + bg(x))dx = a\\int f(x)dx + b\\int g(x)dx$\n",
    "- **Inverse Power Rule**: $\\int x^ndx = \\frac{1}{n+1}x^{n+1} + c$\n",
    "- **Exponential**: $\\int e^xdx = e^x + c$\n",
    "- **$1/x$**: $\\int \\frac{1}{x}dx = \\ln(x) + c$\n",
    "- **Trig**:\n",
    "  - $\\int \\cos(x)dx = \\sin(x) + c$\n",
    "  - $\\int \\sin(x)dx = -\\cos(x) + c$\n",
    "  - $\\int \\sec^2(x)dx = \\tan(x) + c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "Linear Algebra deals with **vectors**, **vector spaces**, and **matrices**.  It is the workhorse underlying so many of the techniques in Machine Learning that we cover as well as the tools that make it all possible.  `numpy` and `scipy` for instance are a virtual cornucopia of linear algebra goodies. Here we're just going to cover some basics.\n",
    "\n",
    "### Vectors\n",
    "I'd like you to imagine a vector as an **arrow** pointing from the origin of your coordinate system to a point in 2-D space, like so:  \n",
    "<img src='img/vector.png'/>\n",
    "\n",
    "This vector represents the point (2, 3) in the xy-plane and **any** point in that plane could be represented by such a vector.  Now take a little imaginative leap and picture extending beyond the 2-D plane to arrows in 3-D, the concept is still the same!  Even better, it's still the same idea in any number of dimensions though, sadly, us humans can't visualize it well.\n",
    "\n",
    "#### Some Notes about Vectors\n",
    "- A vector has a **magnitude**: the length of your \"arrow\"\n",
    "- A vector has a **direction**: the direction your \"arrow\" is pointing\n",
    "- An n-dimensional vector will always have n components\n",
    "  - Each individual component represents the value along each of your coordinate axes (x, y, z, etc)\n",
    "- We almost always refer to our **target variables** as a **vector of targets**\n",
    "  - One row per observation\n",
    "  \n",
    "**Note for Completeness**: The arrow visualization is actually just an abstraction.  In general, vectors in a vector space need only satisfy a certain set of **axioms**, and the arrow visualization just happens to be useful for the types of vectors we'll be dealing with.\n",
    "\n",
    "##### Notation\n",
    "- You may see vectors written many different ways, for instance:\n",
    "  - With an arrow on top (common in science/engineering): $\\vec{\\mathbf{x}} = (2, 3)$\n",
    "  - Like a matrix column: $\\mathbf{x} = \\begin{bmatrix}2\\\\3\\end{bmatrix}$\n",
    "  - Like a matrix row with a transpose: $\\mathbf{x} = \\begin{bmatrix}2 & 3\\end{bmatrix}^T$\n",
    "\n",
    "#### Vector Operations\n",
    "- There are a number of operations on vectors you should be aware of.\n",
    "- This list is non-exhaustive\n",
    "\n",
    "##### Dot (Inner) Products\n",
    "- Consider 2 vectors of the same length ($n$): $\\mathbf{x_1}$ and $\\mathbf{x_2}$:\n",
    "  - The dot/inner product is given by:\n",
    "$$\n",
    "\\mathbf{x_1}^T\\mathbf{x_2} = \\mathbf{x_2}^T\\mathbf{x_1} = \\sum\\limits_{i=1}^{n}x_{1,i}x_{2,i}\n",
    "$$\n",
    "  - e.g.: $\\mathbf{x_1} = \\begin{bmatrix} 2 \\\\ 3\\end{bmatrix}$ and $\\mathbf{x_2} = \\begin{bmatrix} 5 \\\\ 4 \\end{bmatrix}$\n",
    "  - $\\mathbf{x_1}^T\\mathbf{x_2} = 2\\times5 + 3\\times4 = 22$\n",
    "- Inner products between vectors more parallel than anti-parallel are positive\n",
    "- If more anti-parallel, they are negative\n",
    "\n",
    "##### Vector Norm\n",
    "- Assigns a \"length-like\" value to a vector\n",
    "- The **L2 Norm** or **Euclidean Norm** is the **magnitude** of the vector\n",
    "  - It looks like the familiar distance formula: $\\lVert\\mathbf{x}\\rVert = \\sqrt{\\sum\\limits_{i}x_i^2}$\n",
    "\n",
    "##### Cosine Similarity\n",
    "- Cosine similarity normalizes the inner product\n",
    "- Userful for getting normalized similarity scores between vectors\n",
    "- Basically reflects the angle between the vectors\n",
    "- Invariant to vector magnitude, unlike inner product\n",
    "- Given by:\n",
    "  - $\\cos(\\mathbf{x_1}\\mathbf{x_2}) = \\frac{\\mathbf{x_1}^T\\mathbf{x_2}}{\\lVert\\mathbf{x_1}\\rVert\\lVert\\mathbf{x_2}\\rVert}$\n",
    "\n",
    "### Vector Spaces\n",
    "In its simplest terms, a Vector Space is a **set of possible vectors**.  For instance, the 2-D plane from the earlier example forms a Vector Space in 2 dimensions.  \n",
    "- Vector Spaces are characterized by their **# of dimensions**: basically the number of independent directions in the space\n",
    "  - e.g. 2 for 2-D\n",
    "- For our purposes, think of vector spaces as Cartesian Coordinate systems generalized out to n dimensions, in which vectors of features (usually) can land\n",
    "\n",
    "### Matrices\n",
    "- Column vectors are basically just matrices with 1 column\n",
    "- Matrix dimensions are often written as $m \\times n$\n",
    "- We almost always refer to our **features** as a **data matrix of features**\n",
    "  - One row per observation\n",
    "  - One column per feature\n",
    "  \n",
    "##### Notation:\n",
    "- Matrices usually bold capital letters\n",
    "  - e.g.: $\\mathbf{X} = \\begin{bmatrix}a & b\\\\c & d\\end{bmatrix}$\n",
    "- Elements often specified by **row, column indexes**\n",
    "  - e.g.: $\\mathbf{X}_{12}=d$\n",
    "- The dimensions will be represented by dim$(\\mathbf{X})$\n",
    "- Row $i$ is referenced by: $\\mathbf{X}_i$\n",
    "- Column $j$ is referenced by: $\\mathbf{X}^{T}_j$\n",
    "- Transpose: $\\mathbf{X}^T$\n",
    "\n",
    "#### Matrix Operations\n",
    "The following operations with matrices should be familiar for you as a data scientist:\n",
    "\n",
    "##### Addition/Subtraction\n",
    "- To add or subtract matrices you simply line them up and add/subtract the corresponding elements\n",
    "- To add/subtract 2 matrices $\\mathbf{A}$ and $\\mathbf{B}$, then dim$(\\mathbf{A})$ must equal dim$(\\mathbf{B})$\n",
    "- If $\\mathbf{C}=\\mathbf{A} \\pm \\mathbf{B}$, then $\\mathbf{C}_{ij} = \\mathbf{A}_ij \\pm \\mathbf{B}_ij \\; \\forall \\; i,j$\n",
    "\n",
    "##### Multiplication\n",
    "- To multiple 2 matrices $\\mathbf{AB}=\\mathbf{C}$, then $\\mathbf{C}_{ij}$ is the inner product of the $i$th row of $\\mathbf{A}$ with the $j$th column of $\\mathbf{B}$ for all $i, j$\n",
    "- **Order Matters**\n",
    "- **Dimensionality Matters**\n",
    "  - To be legal: # of columns of of $\\mathbf{A}$ must equal # of rows of $\\mathbf{B}$\n",
    "  - dim$(\\mathbf{C})$ will be the # of rows of $\\mathbf{A}$ by # of columns of $\\mathbf{B}$\n",
    "- e.g.:\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{B}=\\begin{bmatrix}a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{bmatrix}\\begin{bmatrix}b_{11} & b_{12} & b_{13} & b_{14} \\\\ b_{21} & b_{22} & b_{23} & b_{24}\\end{bmatrix} = \\begin{bmatrix}a_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22} & a_{11}b_{13}+a_{12}b_{23} & a_{11}b_{14}+a_{12}b_{24} \\\\ a_{21}b_{11}+a_{22}b_{21} & a_{21}b_{12}+a_{22}b_{22} & a_{21}b_{13}+a_{22}b_{23} & a_{21}b_{24}+a_{22}b_{24} \\\\ a_{31}b_{11}+a_{32}b_{21} & a_{31}b_{12}+a_{32}b_{22} & a_{31}b_{13}+a_{32}b_{23} & a_{31}b_{14}+a_{32}b_{24}\\end{bmatrix} = \\mathbf{C}\n",
    "$$\n",
    "\n",
    "##### Transpose\n",
    "- Exchanges rows and columns for matrix $\\mathbf{A}$\n",
    "- e.g.:\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\end{bmatrix} \\Rightarrow \\mathbf{A}^T = \\begin{bmatrix} a & d \\\\ b & e \\\\ c & f \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### Inverse\n",
    "**The Identity Matrix**  \n",
    "- The special **square** matrix $\\mathbf{I}$ is called the Identity Matrix\n",
    "- It has 1s on the main diagonal and 0s elsewhere\n",
    "- e.g. in 2-D: $\\mathbf{I} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "- For any matrix (of legal dimension) $\\mathbf{A}$, $\\mathbf{A}\\mathbf{I} = \\mathbf{A}$\n",
    "- If a matrix $\\mathbf{A}$ is **invertible**:\n",
    "  - $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$\n",
    "  - $\\mathbf{A}^{-1}$ is the **Inverse** matrix of $\\mathbf{A}$\n",
    "\n",
    "#### Matrices as Operators on Vectors\n",
    "- Matrices can be thought of as **operators** that **transform vectors** in a **vector space** (remember your \"arrow\" picture of vectors)\n",
    "- This means stretching them, rotating them, etc\n",
    "- e.g.: The matrix $c\\mathbf{I}$ will stretch any vector in the space by a constant factor of $c$\n",
    "- e.g.: The matrix $\\mathbf{R}=\\begin{bmatrix} cos(\\theta) & -sin(\\theta) \\\\ sin(\\theta) & cos(\\theta) \\end{bmatrix}$ will rotate any vector in the 2-D plane by $90^\\circ$ counter-clockwise\n",
    "\n",
    "### Vector/Matrix Calculus\n",
    "Our rules for calculus of single and multiple variables can generally be extended to vectors and matrices, a great thing for us because many of the **cost functions** we seek to minimize in training algorithms will be equations in vectors and matrices.  Thus, it would be great if we had some rules for taking derivatives of those guys!\n",
    "\n",
    "#### The Gradient Vector\n",
    "- The **Gradient** is a **vector** :)\n",
    "- For a function $f(x_1, x_2, \\cdots , x_n)$ of multiple variables $x_1, x_2, \\cdots, x_n$:\n",
    "  - The gradient is represented by $\\vec{\\nabla}f$\n",
    "  - $\\vec{\\nabla}f = (\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots , \\frac{\\partial f}{\\partial x_n})$\n",
    "- The **direction** of the gradient vector is **in the direction of steepest increase** of $f$\n",
    "- The **magnitude** represents the **slope** of that increase\n",
    "- Thus, to find the **maximum we just follow the gradient**!\n",
    "- Or more likely, to find the **minimum** we just **go in the opposite direction**!  Aka, follow $-\\vec{\\nabla}$\n",
    "\n",
    "#### Derivatives of Matrices\n",
    "In the right situation, we can take **derivatives of matrices with respect to scalars** just like we do with our regular scalar calculus!  So...we **can find optima of functions of matrices**.\n",
    "\n",
    "##### Matrix Derivative Identities\n",
    "Stolen from Wikipedia (I ran out of time!), here are some main ones:\n",
    "<img src='img/matrix.png'/>\n",
    "\n",
    "##### Derivation of Linear Regression\n",
    "Remember the cost function we used for **Linear Regression**, the **Sum of Squared Errors**?  We can write that down in pretty simple matrix form, differentiate the resulting matrix equation and find the $\\beta$s that minimize the cost via that same calculus procedure.\n",
    "\n",
    "Tomorrow we will do just that ;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Geometry\n",
    "One last thing before we go.  For at least one of our algorithms, it will be helpful to understand how the equations of lines and planes can be written and extended from 1 to 2 to n dimensions.  \n",
    "- In 1 dimension, you're famiiar with the equation $y = mx + b$\n",
    "  - Let's rename $y$ to $x_2$ and $x$ to $x_1$\n",
    "  - With some rearranging, we have: $c_1x_1 + c_2x_2 = d$, where $c_1$, $c_2$ and $d$ are some constants\n",
    "  - This general linear equation extends out to as many dimensions as we want!\n",
    "- In 1 dimension, it is a line\n",
    "- In 2 dimensions, it is a plane:\n",
    "  - $c_1x_1 + c_2x_2 + c_3x_3 = d$\n",
    "- In n dimensions, it is a **hyperplane**\n",
    "- If we think of our variables as a **vector** in n-dimensional space, $\\mathbf{x} = (x_1, x_2, \\cdots , x_n)^T$, and the constants $c_i$ as a vector of constants $\\mathbf{c}$, then:\n",
    "  - $\\mathbf{x}^T\\mathbf{c} = d$\n",
    "  - This is the equation for a hyperplane in n dimensions!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
